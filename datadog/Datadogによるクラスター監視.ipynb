{"cells":[{"cell_type":"markdown","source":["## DatadogエージェントのインストールとSparkとシステムのモニタリング\n\nここで作成するinit scriptをクラスターに設定することで、Datadogエージェントをインストールし、SparkメトリクスとログをDatadogに送信するようになります。<br><br>\n\n1. [Datadog](https://www.datadoghq.com/)でアカウントを作成し、APIキーを取得します。以下の画面の`DD_API_KEY=`以降がAPIキーです。\n![](https://sajpstorage.blob.core.windows.net/demo20210501-datadog/get-API-key.png)\n1. DBFS上にinit scriptを作成するので、以下のセルの4行目の`<init-script-folder>`に格納先を指定してください。\n1. 以下のセルを実行し、`datadog-install-driver-workers.sh`を作成します。\n1. クラスターのinit scriptとして、`datadog-install-driver-workers.sh`を指定します。\n![](https://sajpstorage.blob.core.windows.net/demo20210501-datadog/set-init-script.png)\n1. クラスターの**Advanced Options**の**Spark > Environment Variables**にDatadogのAPIキーを、`DD_API_KEY=<your-api-key>`の形式で指定します。\n![](https://sajpstorage.blob.core.windows.net/demo20210501-datadog/Env.png)\n1. クラスターの起動に合わせてエージェントが起動し、Datadogにメトリクスとログが送信されます。\n![](https://sajpstorage.blob.core.windows.net/demo20210501-datadog/Dashboards.png)\n![](https://sajpstorage.blob.core.windows.net/demo20210501-datadog/Datadog-logs.png)\n\nDatabricks参考情報\n- [Manage clusters \\| Databricks on AWS](https://docs.databricks.com/clusters/clusters-manage.html#view-cluster-logs)\n- [Apache Spark Cluster Monitoring with Databricks and Datadog \\- The Databricks Blog](https://databricks.com/blog/2017/06/01/apache-spark-cluster-monitoring-with-databricks-and-datadog.html)\n\nDatadog参考情報\n- [Databricks](https://docs.datadoghq.com/integrations/databricks/?tab=driveronly)\n- 最新のinit scriptはこちらから取得できます。 [Spark](https://docs.datadoghq.com/integrations/spark/?tab=host)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"716298e0-16c7-48db-ae3b-85624566f010"}}},{"cell_type":"markdown","source":["デフォルトではログが出力されないので、`echo \"logs_enabled: true\" >> /etc/datadog-agent/datadog.yaml`でログを有効化する必要があります(31行目)。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2339e3d8-995d-40ec-91cc-90ffdbbf9630"}}},{"cell_type":"code","source":["# 例\n# dbfs:/FileStore/shared_uploads/takaaki.yayoi@databricks.com/datadog-install-driver-workers.sh\n  \ndbutils.fs.put(\"dbfs:/<init-script-folder>/datadog-install-driver-workers.sh\",\"\"\"\n#!/bin/bash\ncat <<EOF >> /tmp/start_datadog.sh\n\n#!/bin/bash\n  \n  hostip=$(hostname -I | xargs)\n\nif [[ \\${DB_IS_DRIVER} = \"TRUE\" ]]; then\n\n  echo \"Installing Datadog agent in the driver (master node) ...\"\n  # CONFIGURE HOST TAGS FOR DRIVER\n  DD_TAGS=\"environment:\\${DD_ENV}\",\"databricks_cluster_id:\\${DB_CLUSTER_ID}\",\"databricks_cluster_name:\\${DB_CLUSTER_NAME}\",\"spark_host_ip:\\${SPARK_LOCAL_IP}\",\"spark_node:driver\"\n\n  # INSTALL THE LATEST DATADOG AGENT 7 ON DRIVER AND WORKER NODES\n  DD_AGENT_MAJOR_VERSION=7 DD_API_KEY=\\$DD_API_KEY DD_HOST_TAGS=\\$DD_TAGS bash -c \"\\$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script.sh)\"\n  \n  # WAIT FOR DATADOG AGENT TO BE INSTALLED\n  while [ -z \\$datadoginstalled ]; do\n    if [ -e \"/etc/datadog-agent/datadog.yaml\" ]; then\n      datadoginstalled=TRUE\n    fi\n    sleep 2\n  done\n  echo \"Datadog Agent is installed\"\n\n  # ENABLE LOGS IN datadog.yaml TO COLLECT DRIVER LOGS\n  echo \"logs_enabled: true\" >> /etc/datadog-agent/datadog.yaml\n\n  while [ -z \\$gotparams ]; do\n    if [ -e \"/tmp/driver-env.sh\" ]; then\n      DB_DRIVER_PORT=\\$(grep -i \"CONF_UI_PORT\" /tmp/driver-env.sh | cut -d'=' -f2)\n      gotparams=TRUE\n    fi\n    sleep 2\n  done\n\n  # WRITING CONFIG FILE FOR SPARK INTEGRATION WITH STRUCTURED STREAMING METRICS ENABLED\n  # MODIFY TO INCLUDE OTHER OPTIONS IN spark.d/conf.yaml.example\n  echo \"init_config:\ninstances:\n    - spark_url: http://\\${DB_DRIVER_IP}:\\${DB_DRIVER_PORT}\n      spark_cluster_mode: spark_driver_mode\n      cluster_name: \\${hostip}\n      streaming_metrics: true\nlogs:\n    - type: file\n      path: /databricks/driver/logs/*.log\n      source: databricks\n      service: databricks\n      log_processing_rules:\n        - type: multi_line\n          name: new_log_start_with_date\n          pattern: \\d{2,4}[\\-\\/]\\d{2,4}[\\-\\/]\\d{2,4}.*\" > /etc/datadog-agent/conf.d/spark.yaml\nelse\n\n  # CONFIGURE HOST TAGS FOR WORKERS\n  DD_TAGS=\"environment:\\${DD_ENV}\",\"databricks_cluster_id:\\${DB_CLUSTER_ID}\",\"databricks_cluster_name:\\${DB_CLUSTER_NAME}\",\"spark_host_ip:\\${SPARK_LOCAL_IP}\",\"spark_node:worker\"\n\n  # INSTALL THE LATEST DATADOG AGENT 7 ON DRIVER AND WORKER NODES\n  DD_AGENT_MAJOR_VERSION=7 DD_API_KEY=\\$DD_API_KEY DD_HOST_TAGS=\\$DD_TAGS bash -c \"\\$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script.sh)\"\n\nfi\n\n  # RESTARTING AGENT\n  sudo service datadog-agent restart\nEOF\n\n# CLEANING UP\nchmod a+x /tmp/start_datadog.sh\n/tmp/start_datadog.sh >> /tmp/datadog_start.log 2>&1 & disown\n\"\"\", True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83ff6a49-06d9-4cba-bec0-fd1bf1383db3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Wrote 2467 bytes.\nOut[1]: True</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote 2467 bytes.\nOut[1]: True</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Datadogによるクラスター監視","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1977302370937636}},"nbformat":4,"nbformat_minor":0}
