{"cells":[{"cell_type":"markdown","source":["# Databricksでデータ分析のAtoZをご紹介\n\nデータ分析におけるAtoZとは何でしょうか？\n![](https://sajpstorage.blob.core.windows.net/workshop20210428-jedai/DA-A-Z.jpg)\n\n- データ分析はビジネス上の課題を解決するための手段の一つです。\n- データ分析というと予測モデル構築が脚光を浴びがちですが、データをビジネス価値につなげる長い道のりのほんの一部です。\n- 本日は、データをビジネス価値創出につなげる道のりを、実例含めてご紹介いたします。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da478a4c-424f-4fdf-97c1-142e6e2e76d3"}}},{"cell_type":"markdown","source":["## 自己紹介\n![](https://sajpstorage.blob.core.windows.net/workshop20210428-jedai/self_introduction_v1.png)\n\nQiitaに色々投稿してます。\n- [Databricksクイックスタートガイド \\- Qiita](https://qiita.com/taka_yayoi/items/125231c126a602693610)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5f4696d-a875-4064-99a1-acb12f85be11"}}},{"cell_type":"markdown","source":["## データ分析の(終わり無き)長い道のり\n個人的経験も踏まえたものです。最後までたどり着けないプロジェクトもたくさんありました...\n<br><br>\n1. ビジネス課題の特定\n1. データ分析における仮説の立案\n1. データ分析アプローチの検討\n1. データソースの調査、分析データの入手\n1. 分析データの読み込み\n1. 探索的データ分析(EDA:Exploratory Data Analysis)\n1. 分析データの前処理\n1. 分析アルゴリズムの検討\n1. 分析パイプラインのレビュー\n1. モデルの構築\n1. モデルの評価\n1. モデルのチューニング\n1. モデルのデプロイ\n1. 精度・性能のモニタリング"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0c1495d-1222-453e-aebe-dbdd897f1a70"}}},{"cell_type":"markdown","source":["## ステップ1〜4 ヒアリング + ホワイトボードを前にしたディスカッション\nいきなりデータを触り始めるプロジェクトはまず存在しません。データ分析には必ずビジネスにつながる目的があるべきです。\n- ステップ1 ビジネス課題の特定<br>\n> あるマーケティング担当者の悩み 「マーケティングを効率的に進めるために、年収の高いお客様を簡単に特定できないだろうか？」<br>\n**ビジネス課題： 富裕層を特定することによるマーケティングの効率化**<br>\n\n- ステップ2 データ分析における仮説の立案\n> あるデータサイエンティストの思い 「デモグラフィック情報から収入を予測できれば、ユーザー情報登録時に年収を予測できるのではないか？」<br>\n**データ分析における仮説: デモグラフィック情報から年収を予測できる**<br>\n\n- ステップ3 データ分析アプローチの検討\n> マーケティング担当者とデータサイエンティストの議論 「具体的な年収を予測するのではなく、年収が一定額以上か未満かを識別するだけで十分ではないか」<br>\n**データ分析アプローチ: 年収が5万ドル以上か否かを分類する二値分類問題に取り組む**<br>\n\n- ステップ4 データソースの調査、分析データの入手\n> データサイエンティストとDWH担当者の会話 「過去に蓄積したデモグラフィック情報と年収情報は利用できそうだ」<br>\n**分析データ: 過去に蓄積したデモグラフィック情報、年収情報**\n\nちなみに意外と大変なのは、ステップ4において、データの由来、スキーマ、更新頻度などの確認です。いろんな人に聞いて回らないとわからないケースも。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d86ff191-a0a7-4449-b767-a759c6760c14"}}},{"cell_type":"markdown","source":["## ステップ5 データの読み込み\n\n上で述べたとおり、デモグラフィック情報に基づいて、年収が5万ドル以上か否かを分類する二値分類問題に取り組みます。データセットは、[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Adult)にあるものを利用します。このデータは既にDatabricksランタイムに格納されています。このノートブックでは、データ処理、機械学習パイプライン、機械学習アルゴリズムなどのMLlibの機能をデモンストレーションします。\n\nその前に、Databricksワークスペースの画面構成を簡単にご説明します。\n\n参考情報：\n- [Databricksワークスペースのコンセプト \\- Qiita](https://qiita.com/taka_yayoi/items/78bf647c40a906d90db0)\n- [Databricksノートブックを使う \\- Qiita](https://qiita.com/taka_yayoi/items/dfb53f63aed2fbd344fc)\n- [Databricksにおけるデータのインポート、読み込み、変更 \\- Qiita](https://qiita.com/taka_yayoi/items/4fa98b343a91b8eaa480)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"342e4420-f310-4a9a-ad5d-e57db2ff1975"}}},{"cell_type":"markdown","source":["大抵の場合、分析データを入手した後にすることはデータの中身の確認でしょう。Databricksでは柔軟にノートブック上での作業を行えるように、多くの`マジックコマンド`がサポートされています。下のセルにある`%fs`もその一つです。Databricksのファイルシステムに格納されているファイルの一部を表示します。\n\n参考情報： \n- [Databricksの言語マジックコマンド](https://qiita.com/taka_yayoi/items/dfb53f63aed2fbd344fc#%E6%B7%B7%E6%88%90%E8%A8%80%E8%AA%9E)\n- [Databricksにおけるファイルシステム \\- Qiita](https://qiita.com/taka_yayoi/items/e16c7272a7feb5ec9a92)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"420dcd62-1273-4d39-a8ea-29b30f4ff5c9"}}},{"cell_type":"code","source":["%fs head --maxBytes=1024 databricks-datasets/adult/adult.data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25ec9816-9c89-4714-abc8-e98289138381"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["データセットにはカラム名が含まれていないため、カラム名とデータタイプを指定するスキーマを作成します。作成したスキーマを指定してCSVファイルを読み込みます。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b396b8c-2622-49a1-955e-ad8d6cc372ea"}}},{"cell_type":"code","source":["# スキーマの定義\nschema = \"\"\"`age` DOUBLE,\n`workclass` STRING,\n`fnlwgt` DOUBLE,\n`education` STRING,\n`education_num` DOUBLE,\n`marital_status` STRING,\n`occupation` STRING,\n`relationship` STRING,\n`race` STRING,\n`sex` STRING,\n`capital_gain` DOUBLE,\n`capital_loss` DOUBLE,\n`hours_per_week` DOUBLE,\n`native_country` STRING,\n`income` STRING\"\"\"\n\n# ファイルを読み込みます\ndataset = spark.read.csv(\"/databricks-datasets/adult/adult.data\", schema=schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0ef9d64-357c-4767-bf61-4a5d42a4c6b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## ステップ6 探索的データ分析(EDA:Exploratory Data Analysis)\n\n個人的にはデータを理解するEDAは非常に重要だと思っています。EDAを通じて取り扱うデータの素性を理解することで、以降の分析での手戻りを減らすことができます。\n\n参考情報：\n- [Databricksにおけるデータの可視化 \\- Qiita](https://qiita.com/taka_yayoi/items/36a307e79e9433121c38)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13d09615-7426-46b0-85f6-44dcabac69db"}}},{"cell_type":"markdown","source":["モデル構築、評価に向けて、データセットをランダムにトレーニングデータとテストデータに分割します。また、再現性確保のために乱数のシードを設定しています。\n\nあらゆる前処理を実行する前の**生の状態**でデータを分割すべきです。これにより、モデルを評価する際、テストデータが未知のデータに近い状態を維持することができます。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0ab14da-2d1f-4b0f-86fa-ff5d3d7a33a1"}}},{"cell_type":"code","source":["trainDF, testDF = dataset.randomSplit([0.8, 0.2], seed=42)\nprint(\"トレーニングデータ:\", trainDF.cache().count()) # 何回かトレーニングするのでデータをキャッシュします\nprint(\"テストデータ:\", testDF.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80f95c02-5fa9-49ee-95d1-81c873d10c93"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["データを確認しましょう。EDA! EDA!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97ad2af6-c07c-4925-a2e3-9413b84a8589"}}},{"cell_type":"code","source":["# データを確認するには、とにかくdisplay!\ndisplay(trainDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37907b60-7704-44d2-8cbe-2a6c63c31622"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["`hours_per_week`(週当たりの勤務時間)の分布はどうなっているでしょうか？"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"774ab06c-ed6d-471b-a692-3708b0428921"}}},{"cell_type":"code","source":["display(trainDF.select(\"hours_per_week\").summary())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"676ce08f-6d6d-42b9-a2c9-4521871b4782"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["`education`(最終学歴)はどうなっているでしょうか？\n\n表を見て、全体の傾向を把握するのには限界があります。百のテーブルは一のグラフにしかずです。そんな時にはlet's可視化！![](https://docs.databricks.com/_images/chart-button.png)をクリック！"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9256c04-482e-4c2e-b730-8a4ee8a532c7"}}},{"cell_type":"code","source":["# 最終学歴でグルーピングして件数をカウント、カウントの昇順でソートして表示\ndisplay(trainDF\n        .groupBy(\"education\")\n        .count()\n        .sort(\"count\", ascending=False))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e086c569-fcd0-407f-a929-8c55587c738d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## インターミッション: Transformers、estimators、pipelines\n\n本ノートブックで説明するMLlibの機械学習における重要な3つのコンセプトは、**Transformers**、**Estimators**、そして、**Pipelines**です。\n<br>\n- **Transformer**: データフレームをインプットとして新たなデータフレームを返却します。Transformersは、データから学習は行わず、モデル学習のためのデータを準備するか、学習したMLlibモデルで予測を行うために、単にルールベースの変換処理を適用します。`.transform()`メソッドでtransformerを呼び出すことができます。\n\n- **Estimator**: `.fit()`メソッドを用いてデータフレームからパラメーターを学習(fit)し、モデルを返却します。モデルはtransformerです。\n\n- **Pipeline**: 複数のステップを容易に実行できるように単一のワークフローにまとめます。機械学習モデル作成には、多くのケースで異なるステップが含まれ、それらを繰り返す必要があります。パイプラインを用いることでこのプロセスを自動化することができます。\n\n参考情報:\n- [ML Pipelines(英語)](https://spark.apache.org/docs/latest/ml-pipeline.html#ml-pipelines)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"389e4fe1-b2ca-4d29-aaa1-9301d066f59d"}}},{"cell_type":"markdown","source":["## ステップ7 分析データの前処理\n\nここでのゴールは、データセットに含まれる特徴量(教育レベル、既婚・未婚、職業など)から、年収`income`のレベルを予測するというものです。まず、MLlibで利用できるように特徴量を操作、前処理を行います。いわゆる、特徴量エンジニアリングです。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12b9e009-b1ee-4114-8592-95de9148016a"}}},{"cell_type":"markdown","source":["### カテゴリー変数を数値に変換する\n\n線形回帰、ロジスティック回帰などの学習アルゴリズムでは、特徴量が数値である必要があります。上記の成人データセットでは、教育、職業、既婚・未婚のデータがカテゴリー変数となっています。\n\n以下のコードでは、カテゴリー変数を0か1のみを取る数値変数に変換するために、どのように`StringIndexer`と`OneHotEncoder`を使用するのかを説明します。\n\n- `StringIndexer`は、文字列のカラムをラベルのインデックスに変換します。例えば、\"red\"、\"blue\"、\"green\"をそれぞれ0、1、2に変換します。 \n- `OneHotEncoder`は、カテゴリー変数のインデックスを二進数のベクトルにマッピングします。当該レコードのカテゴリー変数のインデックスに該当するベクトルの要素に\"1\"が割り当てられます。\n\nSparkにおけるOne-hotエンコーディングは2段階のプロセスとなります。最初にStringIndexerを使い、OneHotEncoderを呼び出します。以下のコードブロックでは、StringIndexerとOneHotEncoderを定義しますが、データにはまだ適用しません。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8de42cfd-7a3e-4e85-9ae6-8ab8a5afa535"}}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, OneHotEncoder\n\ncategoricalCols = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\"]\n\n# 以下の２行はestimatorとなります。後ほどデータセットを変換する際に適用することになる関数を返却します。\nstringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=[x + \"Index\" for x in categoricalCols]) \nencoder = OneHotEncoder(inputCols=stringIndexer.getOutputCols(), outputCols=[x + \"OHE\" for x in categoricalCols]) \n\n# ラベルとなるカラム(\"income\")も(\"<=50K\"、\">50K\")の二つの値をとる文字列のカラムとなります。\n# こちらもStringIndexerを使って数値に変換します。\nlabelToIndex = StringIndexer(inputCol=\"income\", outputCol=\"label\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83adbb0b-4cb5-449e-b0ee-5514b9b41e97"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["このノートブックでは、特徴量エンジニアリングとモデル構築のステップ全てを一つのパイプラインにまとめます。ただ、その前に上のコードブロックで構築した`stringIndexer`estimatorを適用することでestimatorやtransformerがどのように動作するのかを詳しく見てみましょう。\n\nデータセットを変換する`StringIndexerModel`を返却するように`.fit()`メソッドを呼び出します。\n\nそして、`StringIndexerModel`の`.transform()`メソッドを呼び出すことで、特徴量を変換結果を格納するカラム`...Index`が追加された新たなデータフレームが返却されます。必要であれば、表示結果を右にスクロールして追加されたカラムを参照してください。\n\n参考情報:\n- [StringIndexerModel(英語)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/feature/StringIndexerModel.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e335dcfa-e706-4435-80e0-01b90bbae82d"}}},{"cell_type":"code","source":["stringIndexerModel = stringIndexer.fit(trainDF)\ndisplay(stringIndexerModel.transform(trainDF))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7224ade0-55bf-4e74-94aa-8b1bb3bffdfb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 全ての特徴量カラムを一つの特徴量ベクトルにまとめます\n\n多くのMLlibアルゴリズムでは、入力として単一の特徴量カラムが必要となります。それぞれの行の特徴量カラムは、予測に用いる特徴量に対応するベクトルを保持します。\n\nMLlibは、一連のカラムから単一のベクトルカラムを作成する`VectorAssembler`transformerを提供します。\n\n下のコードブロックではどのようにVectorAssemblerを使用するのかを説明します。\n\n参考情報: \n- [VectorAssembler(英語)](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02c3e74d-93e7-4ec2-8453-186389f222b3"}}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\n# ここには、データセットの数値カラムとone-hotエンコードされた２値のベクトル両方が含まれます。\nnumericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\nassemblerInputs = [c + \"OHE\" for c in categoricalCols] + numericCols\nvecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b13cab27-f539-4878-ae28-27da1697cfa5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## ステップ8 分析アルゴリズムの検討\n分析アルゴリズムは、データとビジネス課題に応じて使い分けることになると思います。EDAの過程でも「この辺りのアルゴリズムだろうな」などと当たりをつけながらデータを見ていきます。\n<br><br>\n- お客様を分類したい -> クラスタリング(k-meansや階層型クラスタリング)\n- 値やラベルを予測したい -> 回帰、決定木、ランダムフォレスト、SVM、NN、etc.\n- 画像や動画を判別したい -> ディープラーニングなど\n- 組み合わせを予測したい -> アソシエーション分析\n- テキストを分類したい -> BERTなど"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65ab713c-2921-42f1-8c67-312ccdda5000"}}},{"cell_type":"markdown","source":["## ステップ9 分析パイプラインのレビュー\n\nどのタイミングでレビューを行うのかは、チームやプロジェクトによるかと思います。とは言え、リモートワークが浸透した昨今、ここまで開発したロジックを簡単にシェアできる機能があると便利だと思いませんか？Databricksにはあります！以下の機能を利用いただくことで、チーム間で連携しながら開発を進めることができます。\n<br><br>\n- コメント機能\n- 同時編集機能\n- アクセス権管理機能\n- バージョン管理機能\n- git連携機能\n- アーカイブ機能\n- (地味ですが)セルへのリンク機能"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"903a6d49-09cf-4b38-9cd7-c460cdbc4b0b"}}},{"cell_type":"markdown","source":["## ステップ10 モデルの構築\n\n本ノートブックでは[ロジスティック回帰(英語)](https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression)モデルを使います。\n\n参考情報：\n- [Scikit\\-learn でロジスティック回帰（クラス分類編） \\- Qiita](https://qiita.com/0NE_shoT_/items/b702ab482466df6e5569)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48214b1d-2257-405c-b2f8-9db4b78ecf0a"}}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", regParam=1.0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8be82e10-5eed-48c8-abd5-aa40320b7686"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### パイプラインの構築\n\n`Pipeline`は、transformers、estimatorsが含まれる順番付きのリストです。データセットに適用する変換処理の再現性を確保し、自動化するために、パイプラインを定義することができます。\n\n`StringIndexer`で見たのと同様に、`Pipeline`もestimatorです。`pipeline.fit()`メソッドが、transformerである`PipelineModel`を返却します。\n\n詳細はこちらを参照ください:\n[Pipelines(英語)](https://spark.apache.org/docs/latest/ml-pipeline.html#ml-pipelines)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca360c73-3ad5-45c5-a878-75d1ab9593d1"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\n# これまでに作成したステージを組み合わせてパイプラインを定義します\npipeline = Pipeline(stages=[stringIndexer, encoder, labelToIndex, vecAssembler, lr])\n\n# パイプラインモデルを定義します\npipelineModel = pipeline.fit(trainDF)\n\n# テストデータセットにパイプラインモデルを適用します\npredDF = pipelineModel.transform(testDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1537d9b6-758b-4bc7-904d-6c8f5ebbbf6a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["モデルによる予測結果を表示します。`features`カラムは、one-hotエンコーディングを実行した後、多くのケースで要素のほとんどが0となる[sparse vector(英語)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.linalg.SparseVector.html#pyspark.ml.linalg.SparseVector)となります。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1795c481-2724-4577-91e5-9db268ffc1ae"}}},{"cell_type":"code","source":["display(predDF.select(\"features\", \"label\", \"prediction\", \"probability\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48069fa7-13c7-4531-a450-dcc5f9c36a5b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## ステップ11 モデルの評価\n\n`display`コマンドにはROCカーブを表示するオプションが組み込まれています。\n\n参考情報：\n- [ROC曲線](https://oku.edu.mie-u.ac.jp/~okumura/stat/ROC.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45e92e44-32a8-4435-861b-cf8897646668"}}},{"cell_type":"code","source":["display(pipelineModel.stages[-1], predDF.drop(\"prediction\", \"rawPrediction\", \"probability\"), \"ROC\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9225e1a6-7df7-44ac-b8e7-8bb7d33dff07"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["モデル評価において、ROCカーブのAUC(Area Under the Curve)を計算するために`BinaryClassificationEvaluator`を用い、精度を評価するために`MulticlassClassificationEvaluator`を用います。\n\n参考情報：\n- [曲線下の面積（AUC）](https://oku.edu.mie-u.ac.jp/~okumura/stat/ROC.html)\n> ROC曲線下の面積（Area under the curve， AUC）は分類器（分類のアルゴリズム）の性能の良さを表します。0から1までの値をとり，完全な分類が可能なときの面積は1で，ランダムな分類の場合は0.5になります。\n\n詳細はこちらを参照ください:\n- [BinaryClassificationEvaluator(英語)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.BinaryClassificationEvaluator.html#binaryclassificationevaluator)  \n- [MulticlassClassificationEvaluator(英語)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html#multiclassclassificationevaluator)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1eb27f4-bb8f-4100-86cc-da78bac5a1f6"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n\nbcEvaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\nprint(f\"Area under ROC curve: {bcEvaluator.evaluate(predDF)}\")\n\nmcEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint(f\"Accuracy: {mcEvaluator.evaluate(predDF)}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ca7449a-6961-4ea2-b487-6f4979c17e5a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## ステップ12 モデルのチューニング\n\nMLlibはハイパーパラメーターチューニングと交差検証(cross validation)の手段を提供します。\n- ハイパーパラメータチューニングにおいては、`ParamGridBuilder`を用いることで、モデルのハイパーパラメーターの探索空間を定義できます。\n- 交差検証においては、`CrossValidator`を用いることで、estimator(入力データセットに適用するパイプライン)、evaluator、ハイパーパラメーターの探索空間、交差検証のフォールド数を定義できます。\n\n詳細はこちらを参照ください:\n- [交差検証（クロスバリデーション）とは？合わせてグリッドサーチに関しても学ぼう！ \\| AI Academy Media](https://aiacademy.jp/media/?p=263)\n- [Model selection using cross-validation(英語)](https://spark.apache.org/docs/latest/ml-tuning.html)  \n- [ParamGridBuilder(英語)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html#paramgridbuilder)  \n- [CrossValidator(英語)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html#crossvalidator)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19d0107a-41a7-44e1-ae97-ec05514858b4"}}},{"cell_type":"markdown","source":["モデルをチューニングするために、`ParamGridBuilder`と`CrossValidator`を使用します。本例においては、`CrossValidator`での検証において、3種類の`regParam`、3種類の`elasticNetParam`から生成される、3 x 3 = 9のハイパーパラメーターの組み合わせを使用します。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"994687b5-8c01-445c-9b57-79dae77ce917"}}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .build())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3817b515-2422-4006-b7c7-1c9fe6c6959f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["MLlibの`CrossValidator`を呼び出した際、Databricksは[MLflow](https://mlflow.org/)を用いて、自動的に全てのランを追跡します。MLflowのUI([AWS](https://docs.databricks.com/applications/mlflow/index.html)|[Azure](https://docs.microsoft.com/azure/databricks/applications/mlflow/))を用いて、構築したモデルを比較することができます。下のセルの実行後、画面右上にある**Experiment**ボタンを押してみてください。\n\n本例では、作成したパイプラインをestimatorとします。\n\n<a href=\"https://www.mlflow.org/docs/latest/index.html\"><img width=100 src=\"https://www.mlflow.org/docs/latest/_static/MLflow-logo-final-black.png\" title=\"MLflow Documentation — MLflow 1.15.0 documentation\"></a>\n\n参考情報：\n- [PythonによるDatabricks MLflowクイックスタートガイド \\- Qiita](https://qiita.com/taka_yayoi/items/dd81ac0da656bf883a34)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe1db643-061b-46d4-ac35-95181812e261"}}},{"cell_type":"code","source":["# 3フォールドのCrossValidatorを作成\ncv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=bcEvaluator, numFolds=3, parallelism = 4)\n\nimport mlflow\nimport mlflow.spark\n \nwith mlflow.start_run():\n  # 交差検証の実施。交差検証からベストなモデルを得るために処理に数分かかる場合があります。\n  cvModel = cv.fit(trainDF)\n  \n  # ベストモデルをテストデータで評価しロギングします。\n  test_metric = bcEvaluator.evaluate(cvModel.transform(testDF))\n  mlflow.log_metric('test_' + bcEvaluator.getMetricName(), test_metric) \n  \n  # ベストモデルをロギングします。\n  mlflow.spark.log_model(spark_model=cvModel.bestModel, artifact_path='best-model') "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e101f800-d499-4cb9-930a-5b0bcf9f2c9e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## ステップ13 モデルのデプロイ\n\nベストモデルをデプロイ(配備)して、予測を実行します。デプロイには、いくつかの方法があります。\n<br><br>\n- 特定の場所にモデルをデプロイし、バッチやストリーミングでデータを流し込んで予測を行う\n- モデルサーバーにデプロイし、REST APIなどで予測を行う(モデルサービング)\n\nMLflowを活用することで、モデルサービング及びモデルの本格デプロイに向けた承認フローなどを容易に構築することができます。\n\nここでは、テストデータセットに対する予測を行うために、交差検証によって特定されたベストモデルを用います。\n\n参考情報：\n- [MLflow guide \\| Databricks on AWS(英語)](https://docs.databricks.com/applications/mlflow/index.html)\n- [Track machine learning training runs \\| Databricks on AWS(英語)](https://docs.databricks.com/applications/mlflow/tracking.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbec5dc5-fd9c-4c1b-ba97-6e544eb2138b"}}},{"cell_type":"code","source":["# テストデータセットに対する予測を行うために、交差検証によって特定されたベストモデルを使用\ncvPredDF = cvModel.transform(testDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84455d95-ad0a-4e12-b378-a76f1a858d50"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 予測結果に対する分析"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb92c52d-8d9f-4bd9-b9e5-17ecfe6701e0"}}},{"cell_type":"markdown","source":["予測結果のデータセットを見てみます。`prediction`カラムの値が0の場合、`<=50K`、1の場合`>50K`と予測したことを意味します。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4ebbb3f-8745-4396-a4b6-34bdb054ffec"}}},{"cell_type":"code","source":["display(cvPredDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"200a8f65-f12c-4571-bb2b-df338754da79"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["また、SQLを用いることで、予測結果を年齢別、職業別に集計することができます。SQLを実行するために、予測結果のデータセットから一時ビューを作成します。\n\n参考情報：\n- [Databricksにおけるデータベースおよびテーブル \\- Qiita](https://qiita.com/taka_yayoi/items/e7f6982dfbee7fc84894)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9830ba2-8880-4537-9941-8071cb54baa0"}}},{"cell_type":"code","source":["cvPredDF.createOrReplaceTempView(\"finalPredictions\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3a1f672-dc2b-4811-92e2-a164b7efb3e1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["職業ごとの予測結果\n\n- **0** 年収が`<=50K`\n- **1** 年収が`>50K`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"784cb676-392a-4c80-a62b-82140d359d29"}}},{"cell_type":"code","source":["%sql\nSELECT occupation, prediction, count(*) AS count\nFROM finalPredictions\nGROUP BY occupation, prediction\nORDER BY occupation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"152d55c8-0085-4a00-83a5-321bb02a73d7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["年齢ごとの予測結果\n\n- **0** 年収が`<=50K`\n- **1** 年収が`>50K`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3b34fed-c727-45a6-8c34-d07fab10452c"}}},{"cell_type":"code","source":["%sql\nSELECT age, prediction, count(*) AS count\nFROM finalPredictions\nGROUP BY age, prediction\nORDER BY age"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f150c6e-c89e-42e2-8efa-c1e4b4ebfdec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## ステップ14 精度・性能のモニタリング\n\n実運用においては、モデルが定常的に目標とする精度を達成しているのかをモニタリングする必要があります。モデルの経年劣化「ドリフト」を検知した場合には、再学習を行うなどの対応が必要となります。\n![](https://databricks.com/wp-content/uploads/2019/09/model_drift.jpg)\n\nなお、ドリフトには以下の種類があります。\n\n**概念ドリフト(concept drift)**\n\n> 目標変数の統計的属性が変化した時、予測しようとする本当の概念もまた変化します。例えば、不正トランザクションにおいては、新たな手口が生まれてくると、不正の定義自体を見直さなくてはなりません。このような変化は概念ドリフトを引き起こします。\n\n**データドリフト(data drift)**\n\n> 入力データから選択された特徴量を用いてモデルをトレーニングします。入力データの統計的特性に変化が生じた際、モデルの品質に影響を及ぼします。例えば、季節性によるデータの変化、個人的嗜好の変化、トレンドなどは入力データのドリフトを引き起こします。\n\n**上流データの変化(upstream data changes)**\n\n> モデル品質に影響を与えうるデータパイプライン上流でのオペレーションの変更が生じる場合があります。例えば、特徴量のエンコーディングにおいて華氏から摂氏に変更があったり、特徴量の生成が停止されることでnullや欠損値になるなどです。\n\nここでは、ベストモデルをAUCで評価しますが、実際には評価は一度限りではなく、継続的なオペレーションになります。これが「終わり無き」所以です。\n\n参考情報：\n- [機械学習の本格運用：デプロイメントからドリフト検知まで \\- Qiita](https://qiita.com/taka_yayoi/items/879506231b9ec19dc6a5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7f9e256-10e7-4937-9582-11be20b48b2a"}}},{"cell_type":"code","source":["# AUCと精度を用いてモデルの性能を評価 \nprint(f\"Area under ROC curve: {bcEvaluator.evaluate(cvPredDF)}\")\nprint(f\"Accuracy: {mcEvaluator.evaluate(cvPredDF)}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"856f3570-4f36-4c22-a7ed-8fab899d71b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["本日は、データ分析の終わり無き旅路の一部をお見せしました。\n\nご覧いただけたように、(ホワイトボード、ヒアリングを除く)全ての作業をDatabricksのワークスペースで完結することができます。\n\n是非、Databricksをご利用ください！"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2832b921-4897-4c26-83d0-ea5a0280b54d"}}},{"cell_type":"markdown","source":["# END"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5035f471-27dc-47ee-8ba7-1b342a0c24b9"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Databricksでデータ分析のAtoZをご紹介","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"experimentId":"3219474240587513"},"language":"python","widgets":{},"notebookOrigID":3219474240587513}},"nbformat":4,"nbformat_minor":0}
